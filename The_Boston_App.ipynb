{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ayl7E5lqeADu"
      },
      "source": [
        "The Boston App"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y7a7qBkkVrjY",
        "outputId": "39b4cf1b-e1a9-4014-b8f3-92f6a24d2bbb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/briarmoss/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/briarmoss/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: haversine in /home/briarmoss/.local/lib/python3.10/site-packages (2.8.0)\n",
            "\u001b[33mWARNING: Ignoring invalid distribution -ensorflow (/home/briarmoss/.local/lib/python3.10/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "! pip install haversine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "! pip install --upgrade openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "UuyM154ofHtX",
        "outputId": "8e0c3f7e-d25d-4d59-d09c-af39222e54c3"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "import requests\n",
        "\n",
        "url = 'https://data.boston.gov/api/3/action/package_list'\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful\n",
        "if response.status_code == 200:\n",
        "    data = response.json()  # Convert JSON response to a Python dictionary\n",
        "else:\n",
        "    print(\"Failed to retrieve data\")\n",
        "    data = {}\n",
        "\n",
        "def fetch_datasets(dataset_ids):\n",
        "    with open('datasets_info.txt', 'w') as file:\n",
        "        for dataset_id in dataset_ids:\n",
        "            url = f'https://data.boston.gov/api/3/action/package_show?id={dataset_id}'\n",
        "            response = requests.get(url)\n",
        "\n",
        "            if response.status_code == 200:\n",
        "                data = response.json()\n",
        "                file.write(str(data) + '\\n\\n')\n",
        "            else:\n",
        "                file.write(f'Failed to retrieve data for dataset ID {dataset_id}\\n\\n')\n",
        "\n",
        "# Fetch datasets and write to file\n",
        "#fetch_datasets(data['result'])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "S2B0JkXPhbq1"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def get_dataset_url(resource_id):\n",
        "    base_url = \"https://data.boston.gov/api/3/action/package_show\"\n",
        "    params = {'id': resource_id}\n",
        "    response = requests.get(base_url, params=params)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        # Assuming the first resource in the package is the one we want\n",
        "        return data['result']['resources'][0]['url']\n",
        "    else:\n",
        "        print(\"Failed to get data:\", response.status_code)\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "UvfqFhGAomgR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def load_dataset_into_dataframe(url):\n",
        "    if url:\n",
        "        return pd.read_csv(url)\n",
        "    else:\n",
        "        print(\"Invalid URL\")\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QRdbofuronQp",
        "outputId": "972485e6-627e-4af3-8f58-63aca639127c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "reading Approved Building Permits\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_25770/2775520338.py:35: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(data_url, on_bad_lines='warn')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataframe for Approved Building Permits (CSV) created.\n",
            "reading Food Establishment Inspections\n",
            "Dataframe for Food Establishment Inspections (CSV) created.\n",
            "reading Street Sweeping Schedules\n",
            "Dataframe for Street Sweeping Schedules (CSV) created.\n",
            "reading Big Belly Trash Receptacles\n",
            "Dataframe for Big Belly Trash Receptacles (CSV) created.\n",
            "reading Trash Collection Days\n",
            "Dataframe for Trash Collection Days (CSV) created.\n",
            "reading Boston Street Segments\n",
            "Dataframe for Boston Street Segments (CSV) created.\n",
            "reading City Council Roll Call Votes\n",
            "Dataframe for City Council Roll Call Votes (CSV) created.\n",
            "reading Moving Truck Permits\n",
            "Dataframe for Moving Truck Permits (CSV) created.\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "# List of dataset name IDs\n",
        "datasets = {\n",
        "    'approved-building-permits': 'Approved Building Permits',\n",
        "    'food-establishment-inspections': 'Food Establishment Inspections',\n",
        "    'street-sweeping-schedules': 'Street Sweeping Schedules',\n",
        "    'big-belly-locations': 'Big Belly Trash Receptacles',\n",
        "    'trash-collection-days': 'Trash Collection Days',\n",
        "    'boston-street-segments': 'Boston Street Segments',\n",
        "    'city-council-roll-call-votes': 'City Council Roll Call Votes',\n",
        "    'moving-truck-permits': 'Moving Truck Permits'\n",
        "}\n",
        "\n",
        "\n",
        "dfs = {}\n",
        "\n",
        "# Base URL for the API\n",
        "api_base_url = 'https://data.boston.gov/api/3/action/package_show?id='\n",
        "\n",
        "for dataset_id, dataset_name in datasets.items():\n",
        "    response = requests.get(api_base_url + dataset_id)\n",
        "    if response.status_code == 200:\n",
        "        print(f'reading {dataset_name}')\n",
        "        package = response.json()['result']\n",
        "\n",
        "        # Filter for CSV resources\n",
        "        csv_resources = [resource for resource in package['resources'] if resource['format'].lower() == 'csv']\n",
        "\n",
        "        if csv_resources:\n",
        "            # Assuming the most recent CSV resource is the first in the list\n",
        "            most_recent_csv_resource = csv_resources[0]\n",
        "            data_url = most_recent_csv_resource['url']\n",
        "            df = pd.read_csv(data_url, on_bad_lines='warn')\n",
        "            dfs[dataset_id] = df\n",
        "            print(f'Dataframe for {dataset_name} (CSV) created.')\n",
        "        else:\n",
        "            print(f'No CSV resources found for {dataset_name}.')\n",
        "    else:\n",
        "        print(f'Failed to fetch data for {dataset_name}.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "pefK6jTLtHLH"
      },
      "outputs": [],
      "source": [
        "def extract_columns(dataframes_dict):\n",
        "    columns_dict = {}\n",
        "    for dataset_id, df in dataframes_dict.items():\n",
        "        # Extracting column names from each dataframe and storing them as a list\n",
        "        columns_dict[dataset_id] = df.columns.tolist()\n",
        "    return columns_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "3Wqk4TwVQW6h"
      },
      "outputs": [],
      "source": [
        "columns_dict = extract_columns(dfs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nT-o8gHsQges",
        "outputId": "346f6250-f9d4-4b02-bf96-41987ce594b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'approved-building-permits': ['object_id', 'permitnumber', 'worktype', 'permittypedescr', 'description', 'comments', 'applicant', 'declared_valuation', 'total_fees', 'issued_date', 'expiration_date', 'status', 'owner', 'occupancytype', 'sq_feet', 'address', 'city', 'state', 'zip', 'property_id', 'parcel_id', 'gpsy', 'gpsx', 'geom_2249', 'lat', 'long', 'geom_4326'], 'food-establishment-inspections': ['businessname', 'dbaname', 'legalowner', 'namelast', 'namefirst', 'licenseno', 'issdttm', 'expdttm', 'licstatus', 'licensecat', 'descript', 'result', 'resultdttm', 'violation', 'viollevel', 'violdesc', 'violdttm', 'violstatus', 'statusdate', 'comments', 'address', 'city', 'state', 'zip', 'property_id', 'location'], 'street-sweeping-schedules': ['main_id', 'st_name', 'dist', 'dist_name', 'start_time', 'end_time', 'side', 'from', 'to', 'miles', 'section', 'one_way', 'week_1', 'week_2', 'week_3', 'week_4', 'week_5', 'sunday', 'monday', 'tuesday', 'wednesday', 'thursday', 'friday', 'saturday', 'every_day', 'year_round', 'north_end_pilot', 'timestamp', 'parent', 'losta', 'hista'], 'big-belly-locations': ['description', 'Location'], 'trash-collection-days': ['OBJECTID', 'OBJECTID_1', 'RECOLLECT', 'PWDDIST', 'DT', 'TRASHDAY', 'OBJECTID_2', 'CURR_SCHED', 'FUTR_SCHED', 'C_SCH_END', 'F_SCH_STRT', 'Shape_Leng', 'ORIG_FID', 'Shape_Le_1', 'ShapeSTArea', 'ShapeSTLength'], 'boston-street-segments': ['OBJECTID', 'SHAPE', 'SEGMENT_ID', 'L_F_ADD', 'L_T_ADD', 'R_F_ADD', 'R_T_ADD', 'PRE_DIR', 'ST_NAME', 'ST_TYPE', 'SUF_DIR', 'MUN_L', 'MUN_R', 'CFCC', 'SPEEDLIMIT', 'ONEWAY', 'HEIGHT', 'WEIGHT', 'WIDTH', 'F_ZLEV', 'T_ZLEV', 'SHIELD', 'HWY_NUM', 'NBHD_L', 'NBHD_R', 'FT_COST', 'TF_COST', 'TF_DIR', 'FT_DIR', 'STATE00_L', 'STATE00_R', 'COUNTY00_L', 'COUNTY00_R', 'PLACE00_L', 'PLACE00_R', 'TRACT00_L', 'TRACT00_R', 'BLOCK00_L', 'BLOCK00_R', 'MCD00_L', 'MCD00_R', 'STREET_ID', 'SHAPESTLength'], 'city-council-roll-call-votes': ['roll_call_id', 'docket_number', 'subject', 'vote_date', 'councillor', 'vote'], 'moving-truck-permits': ['permit_number', 'work_type', 'permit_type_descr', 'description', 'comments', 'application_method', 'applicant_city', 'applicant_state', 'applicant_zip', 'is_contractor', 'total_fees', 'issued_date', 'expiration_date', 'status', 'occupancy_type', 'city', 'state', 'zip', 'lat', 'long', 'geom_4326']}\n"
          ]
        }
      ],
      "source": [
        "print(columns_dict)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQl0u1z_RnG8",
        "outputId": "d3d550ea-3b89-492f-c3ac-ce287b0bf9fa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample data from dataset: approved-building-permits\n",
            "Sample from address:\n",
            "0          181-183 State ST\n",
            "1         175 W Boundary RD\n",
            "2            15 Prospect ST\n",
            "3      211 W Springfield ST\n",
            "4    14 William Jackson AVE\n",
            "Name: address, dtype: object\n",
            "Sample from city:\n",
            "0          Boston\n",
            "1    West Roxbury\n",
            "2     Charlestown\n",
            "3         Roxbury\n",
            "4        Brighton\n",
            "Name: city, dtype: object\n",
            "Sample from state:\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    MA\n",
            "1    MA\n",
            "2    MA\n",
            "3    MA\n",
            "4    MA\n",
            "Name: state, dtype: object\n",
            "Sample from zip:\n",
            "0    2109.0\n",
            "1    2132.0\n",
            "2    2129.0\n",
            "3    2118.0\n",
            "4    2135.0\n",
            "Name: zip, dtype: object\n",
            "Sample from gpsy:\n",
            "0    2.956235e+06\n",
            "1    2.920239e+06\n",
            "2    2.962078e+06\n",
            "3    2.949423e+06\n",
            "4    2.950791e+06\n",
            "Name: gpsy, dtype: float64\n",
            "Sample from gpsx:\n",
            "0    777000.467775\n",
            "1    751016.119559\n",
            "2    775710.380542\n",
            "3    769648.312793\n",
            "4    749690.298790\n",
            "Name: gpsx, dtype: float64\n",
            "Sample from geom_2249:\n",
            "0    0101000020C9080000014080EF50B6274128B89653E58D...\n",
            "1    0101000020C908000081DB363D50EB264164AA649F9747...\n",
            "2    0101000020C90800007E6BD6C23CAC2741422F500F4F99...\n",
            "3    0101000020C9080000025726A0E07C274183505E499780...\n",
            "4    0101000020C9080000FCFDFA98F4E02641F6694F594383...\n",
            "Name: geom_2249, dtype: object\n",
            "Sample from lat:\n",
            "0    42.359190\n",
            "1    42.260750\n",
            "2    42.375243\n",
            "3    42.340600\n",
            "4    42.344600\n",
            "Name: lat, dtype: float64\n",
            "Sample from long:\n",
            "0   -71.052924\n",
            "1   -71.149611\n",
            "2   -71.057585\n",
            "3   -71.080251\n",
            "4   -71.154051\n",
            "Name: long, dtype: float64\n",
            "Sample from geom_4326:\n",
            "0    0101000020E6100000A703291D63C351C074AD05ECF92D...\n",
            "1    0101000020E61000005F23793993C951C071ECAA3E6021...\n",
            "2    0101000020E6100000F053B47AAFC351C0A6BB62F20730...\n",
            "3    0101000020E6100000D72A24D322C551C044521DC4982B...\n",
            "4    0101000020E61000009DED6FF7DBC951C0929A5BD71B2C...\n",
            "Name: geom_4326, dtype: object\n",
            "\n",
            "Sample data from dataset: food-establishment-inspections\n",
            "Sample from address:\n",
            "0    55  COURT ST\n",
            "1    55  COURT ST\n",
            "2    55  COURT ST\n",
            "3    55  COURT ST\n",
            "4    55  COURT ST\n",
            "Name: address, dtype: object\n",
            "Sample from city:\n",
            "0    BOSTON\n",
            "1    BOSTON\n",
            "2    BOSTON\n",
            "3    BOSTON\n",
            "4    BOSTON\n",
            "Name: city, dtype: object\n",
            "Sample from state:\n",
            "0    MA\n",
            "1    MA\n",
            "2    MA\n",
            "3    MA\n",
            "4    MA\n",
            "Name: state, dtype: object\n",
            "Sample from zip:\n",
            "0    2108.0\n",
            "1    2108.0\n",
            "2    2108.0\n",
            "3    2108.0\n",
            "4    2108.0\n",
            "Name: zip, dtype: float64\n",
            "Sample from location:\n",
            "0    (42.359227, -71.058878)\n",
            "1    (42.359227, -71.058878)\n",
            "2    (42.359227, -71.058878)\n",
            "3    (42.359227, -71.058878)\n",
            "4    (42.359227, -71.058878)\n",
            "Name: location, dtype: object\n",
            "\n",
            "Sample data from dataset: street-sweeping-schedules\n",
            "Sample from st_name:\n",
            "0       Ackley Pl\n",
            "1    Arcadia Park\n",
            "2     Ashcroft St\n",
            "3     Boylston St\n",
            "4     Chestnut St\n",
            "Name: st_name, dtype: object\n",
            "Sample from from:\n",
            "0    Washington St\n",
            "1        Draper St\n",
            "2       Perkins St\n",
            "3       Tremont St\n",
            "4        Walnut St\n",
            "Name: from, dtype: object\n",
            "Sample from to:\n",
            "0      Dead End\n",
            "1     Ditson St\n",
            "2    Moraine St\n",
            "3    Charles St\n",
            "4    Charles St\n",
            "Name: to, dtype: object\n",
            "\n",
            "Sample data from dataset: big-belly-locations\n",
            "Sample from Location:\n",
            "0    (42.36034013, -71.05581905)\n",
            "1    (42.37344585, -71.03900682)\n",
            "2    (42.36593588, -71.05833609)\n",
            "3          (42.36112, -71.06296)\n",
            "4      (42.3471459, -71.0882423)\n",
            "Name: Location, dtype: object\n",
            "\n",
            "Sample data from dataset: trash-collection-days\n",
            "No location data columns defined for trash-collection-days\n",
            "\n",
            "Sample data from dataset: boston-street-segments\n",
            "No location data columns defined for boston-street-segments\n",
            "\n",
            "Sample data from dataset: city-council-roll-call-votes\n",
            "No location data columns defined for city-council-roll-call-votes\n",
            "\n",
            "Sample data from dataset: moving-truck-permits\n",
            "Sample from city:\n",
            "0    South Boston\n",
            "1          Boston\n",
            "2     Charlestown\n",
            "3         Roxbury\n",
            "4          Boston\n",
            "Name: city, dtype: object\n",
            "Sample from state:\n",
            "0    MA\n",
            "1    MA\n",
            "2    MA\n",
            "3    MA\n",
            "4    MA\n",
            "Name: state, dtype: object\n",
            "Sample from zip:\n",
            "0    02127\n",
            "1    02113\n",
            "2    02129\n",
            "3    02118\n",
            "4    02115\n",
            "Name: zip, dtype: object\n",
            "Sample from lat:\n",
            "0    42.335807\n",
            "1    42.364200\n",
            "2    42.376534\n",
            "3    42.341280\n",
            "4    42.349870\n",
            "Name: lat, dtype: float64\n",
            "Sample from long:\n",
            "0   -71.036260\n",
            "1   -71.052820\n",
            "2   -71.063113\n",
            "3   -71.077960\n",
            "4   -71.087310\n",
            "Name: long, dtype: float64\n",
            "Sample from geom_4326:\n",
            "0    0101000020E6100000CA89761552C251C04A6249B9FB2A...\n",
            "1    0101000020E6100000C824236761C351C0029A081B9E2E...\n",
            "2    0101000020E6100000F5BC1B0B0AC451C081EA1F443230...\n",
            "3    0101000020E61000005B99F04BFDC451C0B2632310AF2B...\n",
            "4    0101000020E610000048A7AE7C96C551C0FFEC478AC82C...\n",
            "Name: geom_4326, dtype: object\n"
          ]
        }
      ],
      "source": [
        "def print_location_data_samples(dfs):\n",
        "    # Define the location-related columns for each dataset\n",
        "    location_columns = {\n",
        "        'approved-building-permits': ['address', 'city', 'state', 'zip', 'gpsy', 'gpsx', 'geom_2249', 'lat', 'long', 'geom_4326'],\n",
        "        'food-establishment-inspections': ['address', 'city', 'state', 'zip', 'location'],\n",
        "        'street-sweeping-schedules': ['st_name', 'from', 'to'],\n",
        "        'big-belly-locations': ['Location'],\n",
        "        'moving-truck-permits': ['city', 'state', 'zip', 'lat', 'long', 'geom_4326'],\n",
        "        # Add other datasets if needed\n",
        "    }\n",
        "\n",
        "    for dataset_id, df in dfs.items():\n",
        "        print(f\"\\nSample data from dataset: {dataset_id}\")\n",
        "\n",
        "        # Check if the dataset has defined location columns\n",
        "        if dataset_id in location_columns:\n",
        "            for col in location_columns[dataset_id]:\n",
        "                if col in df.columns:\n",
        "                    # Print a sample from the column\n",
        "                    print(f\"Sample from {col}:\")\n",
        "                    print(df[col].dropna().head(5))\n",
        "                else:\n",
        "                    print(f\"Column {col} not found in {dataset_id}\")\n",
        "        else:\n",
        "            print(f\"No location data columns defined for {dataset_id}\")\n",
        "\n",
        "# Example usage\n",
        "# Assume 'dfs' is your dictionary containing dataset IDs as keys and DataFrames as values\n",
        "print_location_data_samples(dfs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "vpp8neDpUMHC"
      },
      "outputs": [],
      "source": [
        "def access_approved_building_permits(df):\n",
        "    # Assuming the DataFrame already has 'lat' and 'long' columns\n",
        "    return df\n",
        "\n",
        "def access_location_food_establishment_inspections(df):\n",
        "    # Split the 'location' column into 'lat' and 'long'\n",
        "    df[['lat', 'long']] = df['location'].str.extract(r'\\((.*), (.*)\\)')\n",
        "    df['lat'] = pd.to_numeric(df['lat'], errors='coerce')\n",
        "    df['long'] = pd.to_numeric(df['long'], errors='coerce')\n",
        "    return df\n",
        "\n",
        "def access_location_big_belly_locations(df):\n",
        "    # Split the 'Location' column into 'lat' and 'long'\n",
        "    df[['lat', 'long']] = df['Location'].str.extract(r'\\((.*), (.*)\\)')\n",
        "    df['lat'] = pd.to_numeric(df['lat'], errors='coerce')\n",
        "    df['long'] = pd.to_numeric(df['long'], errors='coerce')\n",
        "    return df\n",
        "\n",
        "def access_moving_truck_permits_location(df):\n",
        "    # Assuming the DataFrame already has 'lat' and 'long' columns\n",
        "    return df\n",
        "\n",
        "# Example usage\n",
        "# Assuming dfs is your dictionary with dataset IDs as keys and DataFrames as values\n",
        "\n",
        "dfs['moving-truck-permits'] = access_moving_truck_permits_location(dfs['moving-truck-permits'])\n",
        "dfs['approved-building-permits'] = access_approved_building_permits(dfs['approved-building-permits'])\n",
        "dfs['food-establishment-inspections'] = access_location_food_establishment_inspections(dfs['food-establishment-inspections'])\n",
        "dfs['big-belly-locations'] = access_location_big_belly_locations(dfs['big-belly-locations'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "p83eJTG4Wk9C"
      },
      "outputs": [],
      "source": [
        "def access_date_approved_building_permits(df):\n",
        "    # Convert 'issued_date' to datetime. If timezone-naive, localize to UTC\n",
        "    df['date'] = pd.to_datetime(df['issued_date'], errors='coerce')\n",
        "    if df['date'].dt.tz is None:\n",
        "        df['date'] = df['date'].dt.tz_localize('UTC')\n",
        "    return df\n",
        "\n",
        "def access_date_food_establishment_inspections(df):\n",
        "    # Convert 'issdttm' to datetime. If timezone-naive, localize to UTC\n",
        "    # for each row check all columns ending in dttm and convert to datetime, then pick the latest one and rename it to date\n",
        "    date_columns = [col for col in df.columns if col.endswith('dttm')]\n",
        "    df[date_columns] = df[date_columns].apply(pd.to_datetime, errors='coerce')\n",
        "    df['date'] = df[date_columns].max(axis=1)\n",
        "    df = df.drop(columns=date_columns)\n",
        "    if df['date'].dt.tz is None:\n",
        "        df['date'] = df['date'].dt.tz_localize('UTC')\n",
        "    \n",
        "    return df\n",
        "\n",
        "def access_date_big_belly_locations(df):\n",
        "    # Assuming this dataset does not have a date column\n",
        "    return df\n",
        "\n",
        "def access_date_moving_truck_permits(df):\n",
        "    # Standardize 'issued_date' to datetime\n",
        "    df['issued_date'] = pd.to_datetime(df['issued_date'], errors='coerce')\n",
        "    if df['issued_date'].dt.tz is None:\n",
        "        df['issued_date'] = df['issued_date'].dt.tz_localize('UTC')\n",
        "    # Standardize 'expiration_date' to datetime\n",
        "    df['expiration_date'] = pd.to_datetime(df['expiration_date'], errors='coerce')\n",
        "    if df['expiration_date'].dt.tz is None:\n",
        "        df['expiration_date'] = df['expiration_date'].dt.tz_localize('UTC')\n",
        "\n",
        "    return df\n",
        "\n",
        "# Example usage\n",
        "dfs['moving-truck-permits'] = access_date_moving_truck_permits(dfs['moving-truck-permits'])\n",
        "dfs['approved-building-permits'] = access_date_approved_building_permits(dfs['approved-building-permits'])\n",
        "dfs['food-establishment-inspections'] = access_date_food_establishment_inspections(dfs['food-establishment-inspections'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "wfCb51ajUUFc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from haversine import haversine\n",
        "\n",
        "def filter_datasets_by_location(dfs, lat, lon, radius, dataset_ids):\n",
        "    def is_within_radius(lat1, lon1, lat2, lon2, radius):\n",
        "        return haversine((lat1, lon1), (lat2, lon2)) <= radius\n",
        "\n",
        "    filtered_dfs = {}\n",
        "    for dataset_id in dataset_ids:\n",
        "        if dataset_id in dfs:\n",
        "            df = dfs[dataset_id]\n",
        "            # Assuming columns 'lat' and 'long' exist and are correctly formatted\n",
        "            if 'lat' in df.columns and 'long' in df.columns:\n",
        "                # Filter the DataFrame\n",
        "                mask = df.apply(lambda row: is_within_radius(lat, lon, row['lat'], row['long'], radius), axis=1)\n",
        "                filtered_dfs[dataset_id] = df[mask]\n",
        "            else:\n",
        "                print(f\"Latitude/Longitude columns not found in {dataset_id}\")\n",
        "\n",
        "    return filtered_dfs\n",
        "\n",
        "# Usage Example\n",
        "# dfs: dictionary with dataset IDs as keys and DataFrames as values\n",
        "# lat, lon: your reference latitude and longitude\n",
        "# radius: radius in kilometers\n",
        "# dataset_ids: list of dataset IDs to filter\n",
        "# result = filter_datasets_by_location(dfs, lat, lon, radius, dataset_ids)\n",
        "\n",
        "#Hilton Boston Park Plaza = 42.3495242,-71.0697654\n",
        "dataset_id_array = ['approved-building-permits','food-establishment-inspections','big-belly-locations','moving-truck-permits']\n",
        "location_filtered_result = filter_datasets_by_location(dfs, 42.345197, -71.077664, .5, dataset_id_array)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3JS9XRqW-pr",
        "outputId": "187da4a5-c34e-44c5-d28b-011852d59083"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "filtering approved-building-permits\n",
            "filtering food-establishment-inspections\n",
            "filtering big-belly-locations\n",
            "No suitable date column found in big-belly-locations, returning original DataFrame.\n",
            "filtering moving-truck-permits\n"
          ]
        }
      ],
      "source": [
        "def filter_datasets_by_date(dfs, start_date, end_date):\n",
        "    filtered_dfs = {}\n",
        "    for dataset_id, df in dfs.items():\n",
        "        print(f'filtering {dataset_id}')\n",
        "        if 'date' in df.columns:\n",
        "            mask = (df['date'] >= start_date) & (df['date'] <= end_date)\n",
        "        elif 'issued_date' in df.columns and 'expiration_date' in df.columns:\n",
        "            mask = ((df['issued_date'] >= start_date) & (df['issued_date'] <= end_date)) | \\\n",
        "                   ((df['expiration_date'] >= start_date) & (df['expiration_date'] <= end_date))\n",
        "        elif 'issued_date' in df.columns:\n",
        "            mask = (df['issued_date'] >= start_date) & (df['issued_date'] <= end_date)\n",
        "        elif 'expiration_date' in df.columns:\n",
        "            mask = (df['expiration_date'] >= start_date) & (df['expiration_date'] <= end_date)\n",
        "        else:\n",
        "            print(f\"No suitable date column found in {dataset_id}, returning original DataFrame.\")\n",
        "            filtered_dfs[dataset_id] = df\n",
        "            continue\n",
        "\n",
        "        filtered_dfs[dataset_id] = df.loc[mask]\n",
        "    return filtered_dfs\n",
        "\n",
        "# Example usage\n",
        "# start_date and end_date should be pandas Timestamp objects\n",
        "# filtered_dfs = filter_datasets_by_date(dfs, start_date, end_date)\n",
        "\n",
        "\n",
        "# Example usage\n",
        "# start_date and end_date should be pandas Timestamp objects\n",
        "# filtered_dfs = filter_datasets_by_date(dfs, start_date, end_date)\n",
        "\n",
        "\n",
        "# Example usage\n",
        "# Assuming dfs contains datasets processed by both location and date access functions\n",
        "start_date = pd.to_datetime('2023-11-20', utc=True)\n",
        "end_date = pd.to_datetime('2023-11-30', utc=True)\n",
        "date_filtered_result = filter_datasets_by_date(location_filtered_result, start_date, end_date)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HymQAl_6V2yU",
        "outputId": "86baee50-bdb1-4487-e16d-e7d7b496550f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved approved-building-permits to output/approved-building-permits.csv\n",
            "Saved food-establishment-inspections to output/food-establishment-inspections.csv\n",
            "Saved big-belly-locations to output/big-belly-locations.csv\n",
            "Saved moving-truck-permits to output/moving-truck-permits.csv\n"
          ]
        }
      ],
      "source": [
        "def save_dfs_to_csv(dfs, folder_path='output/'):\n",
        "    import os\n",
        "\n",
        "    # Create the folder if it doesn't exist\n",
        "    os.makedirs(folder_path, exist_ok=True)\n",
        "\n",
        "    for dataset_id, df in dfs.items():\n",
        "        file_name = f\"{folder_path}{dataset_id}.csv\"\n",
        "        df.to_csv(file_name, index=False)\n",
        "        print(f\"Saved {dataset_id} to {file_name}\")\n",
        "\n",
        "# Usage\n",
        "# Assuming 'dfs' is your dictionary containing the processed data\n",
        "save_dfs_to_csv(date_filtered_result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "JwKJ0PeUpm3S"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def consolidate_csvs_to_text(csv_files, output_file):\n",
        "    with open(output_file, 'w') as outfile:\n",
        "        for dataset_id, csv_file in csv_files.items():\n",
        "            try:\n",
        "                df = pd.read_csv(csv_file)\n",
        "                outfile.write(f\"Dataset ID: {dataset_id}\\n\")\n",
        "                df.to_string(outfile, header=True, index=False)\n",
        "                outfile.write(\"\\n\\n\")  # Adding a couple of new lines for separation\n",
        "            except Exception as e:\n",
        "                outfile.write(f\"Failed to process {csv_file}: {e}\\n\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "yQ2vJjGIpsj2"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "csv_files = {\n",
        "    'approved-building-permits': './output/approved-building-permits.csv',\n",
        "    'food-establishment-inspections': './output/food-establishment-inspections.csv',\n",
        "    'big-belly-locations': './output/big-belly-locations.csv',\n",
        "    'moving-truck-permits' : './output/moving-truck-permits.csv'\n",
        "}\n",
        "consolidate_csvs_to_text(csv_files, 'output/consolidated_data.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOTbN9MrofLk"
      },
      "source": [
        "# Give the data to the Boston GPT for analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n1oPBw6nok91"
      },
      "source": [
        "# Let's try another location: Roxbury YMCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mf1XBprWokdh",
        "outputId": "b5aa4b11-60b3-40bd-d8b1-a3568056055f"
      },
      "outputs": [],
      "source": [
        "#Roxbury YMCA coordinates 42.318389,-71.0849831\n",
        "\n",
        "#YMCA_location_filtered_result = filter_datasets_by_location(dfs, 42.318389, -71.0849831, .25, dataset_id_array)\n",
        "# Example usage\n",
        "# Assuming dfs contains datasets processed by both location and date access functions\n",
        "#start_date = pd.to_datetime('2023-11-20', utc=True)\n",
        "#end_date = pd.to_datetime('2023-11-30', utc=True)\n",
        "#YMCA_date_filtered_result = filter_datasets_by_date(YMCA_location_filtered_result, start_date, end_date)\n",
        "#save_dfs_to_csv(YMCA_date_filtered_result)\n",
        "\n",
        "# Example usage\n",
        "csv_files = {\n",
        "    'approved-building-permits': './output/approved-building-permits.csv',\n",
        "    'food-establishment-inspections': './output/food-establishment-inspections.csv',\n",
        "    'big-belly-locations': './output/big-belly-locations.csv',\n",
        "    'moving-truck-permits' : './output/moving-truck-permits.csv'\n",
        "}\n",
        "#consolidate_csvs_to_text(csv_files, 'output/ymca_consolidated_data.txt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "# Load environment variables from .env file\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ChatCompletionMessage(content=\"The approved-building-permits dataset includes records of approved electrical permits. Some of the activities include electrical work at La Padrona restaurant, installation of wiring at Northeastern University, remodeling of a kitchen, renovation of a space at Flare Capital, installation of a solar PV system, restoration of power due to a fire, and kitchen renovations. These activities may affect people's day-to-day lives by impacting the availability and quality of electrical services.\\n\\nThe big-belly-locations dataset includes geographic coordinates of various locations in Boston where Big Belly waste and recycling containers are located. This may be useful to residents and visitors looking for waste disposal options in the area.\\n\\nLastly, the moving-truck-permits dataset contains records of street occupancy permits for moving trucks in different areas. This might affect traffic and parking availability in the listed locations, potentially impacting local residents and businesses.\", role='assistant', function_call=None, tool_calls=None)\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI()\n",
        "\n",
        "prompt = \"The following is a list of datasets with records pertinent to a central location. Provide a human readable report in english of the activities, with a focus on those that might affect their day to day life\\n\"\n",
        "\n",
        "with open('output/consolidated_data.txt', 'r') as file:\n",
        "    prompt += file.read()\n",
        "\n",
        "completion = client.chat.completions.create(\n",
        "  model=\"gpt-3.5-turbo-1106\",\n",
        "  messages=[\n",
        "    {\"role\": \"user\", \"content\": prompt}\n",
        "  ]\n",
        ")\n",
        "\n",
        "print(completion.choices[0].message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "def save_response_to_file(response, filename):\n",
        "    with open(filename, 'w') as file:\n",
        "        file.write(response)\n",
        "\n",
        "# Example usage\n",
        "response_content = completion.choices[0].message.content  # Assuming completion is your API response\n",
        "save_response_to_file(response_content, 'gpt3_English_response_output.txt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "('The approved-building-permits dataset includes records of approved '\n",
            " 'electrical permits. Some of the activities include electrical work at La '\n",
            " 'Padrona restaurant, installation of wiring at Northeastern University, '\n",
            " 'remodeling of a kitchen, renovation of a space at Flare Capital, '\n",
            " 'installation of a solar PV system, restoration of power due to a fire, and '\n",
            " \"kitchen renovations. These activities may affect people's day-to-day lives \"\n",
            " 'by impacting the availability and quality of electrical services.\\n'\n",
            " '\\n'\n",
            " 'The big-belly-locations dataset includes geographic coordinates of various '\n",
            " 'locations in Boston where Big Belly waste and recycling containers are '\n",
            " 'located. This may be useful to residents and visitors looking for waste '\n",
            " 'disposal options in the area.\\n'\n",
            " '\\n'\n",
            " 'Lastly, the moving-truck-permits dataset contains records of street '\n",
            " 'occupancy permits for moving trucks in different areas. This might affect '\n",
            " 'traffic and parking availability in the listed locations, potentially '\n",
            " 'impacting local residents and businesses.')\n"
          ]
        }
      ],
      "source": [
        "#output the response in human readable format\n",
        "from pprint import pprint\n",
        "\n",
        "pprint(response_content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
